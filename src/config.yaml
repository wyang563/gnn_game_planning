game:
  dt: 0.1
  T_receding_horizon_planning: 20            # Planning horizon for each individual game
  T_receding_horizon_iterations: 50               # Total number of receding horizon iterations
  T_total: 50               # Total number of time steps in trajectory (T_receding_horizon_planning * T_receding_horizon_iterations)
  T_observation: 10         # Number of steps to observe before solving the game

  # Agent configuration
  N_agents: 10              # Total number of agents in the game
  ego_agent_id: 0           # ID of the ego agent (usually 0)
  
  # State and control dimensions
  state_dim: 4              # [x, y, vx, vy] for point mass agents
  control_dim: 2            # [ax, ay] for point mass agents
  
  # Environment parameters
  boundary_size_4p: 2.5
  boundary_size_10p: 3.5

  # initiation type (random positions or collisions at origin)
  initiation_type: "random" # ["random", "origin"]

optimization:
  # iLQGames solver parameters
  num_iters: 50            # Number of iterations for iLQGames solver  
  step_size: 0.005          # Step size for gradient descent
  
  # Cost function weights
  navigation_weight: 1.0    # Weight for navigation cost
  collision_weight: 1.0    # Weight for collision avoidance
  collision_scale: 1.0      # Scale factor in exponential collision cost
  control_weight: 0.1       # Weight for control cost

  # Q, R matrices
  Q: [0.1, 0.1, 0.001, 0.001]
  R: [0.01, 0.01]

reference_generation:
  num_samples: 1000
  reference_gen_type: "fixed" # ["fixed", "variable"] where variable is number of agents is random up to n_agents
  save_dir_fixed: "reference_trajectories_${N_agents}p"
  save_dir_variable: "reference_trajectories_upto_${N_agents}p"

  train_samples: 0.9 # decimals encoding train/test split
  test_samples: 0.1

  # Visualization
  save_plots: true          # Whether to save trajectory plots
  plot_format: "png"        # Plot format ["png", "pdf", "svg"]
  plot_dpi: 300             # Plot resolution

psn:
  # Network architecture
  # Hidden layer dimensions: [64, 32] for 4 players, [128, 64] for 10 players (reduced for GRU version)
  hidden_dims_4p: [64, 32]          # Hidden dimensions for MLP head (reduced for GRU version)
  hidden_dims_10p: [128, 64]        # Hidden dimensions for MLP head (reduced for GRU version)
  gru_hidden_size: 64               # GRU hidden size for temporal processing
  output_dim_per_agent: 8   # Output dimension per other agent
  activation: "relu"        # Activation function
  use_batch_norm: false     # Disabled for GRU version (use dropout instead)
  dropout_rate: 0.3         # Dropout rate (increased for GRU version)
  
  # Training parameters
  learning_rate: 0.001      # Learning rate for Adam optimizer
  batch_size: 32            # Batch size for training
  num_epochs: 100            # Number of training epochs
  
  # Loss function weights
  sigma1: 0.075              # Weight for mask sparsity loss
  sigma2: 0.075               # Weight for mask diversity loss
  
  # Goal source configuration
  # IMPORTANT: This setting affects testing behavior:
  # - If true: PSN training uses true goals, testing should also use true goals
  # - If false: PSN training uses goal inference model, testing should also use goal inference model
  # The testing script will automatically detect this setting to ensure consistency
  use_true_goals: true     # Whether to use true goals instead of predicted goals from goal inference model
  
  # Observation input options
  obs_input_type: "full"    # Observation input type ["full", "partial"]
  # - "full": Use full state (x, y, vx, vy) - 4 dimensions per agent
  # - "partial": Use position only (x, y) - 2 dimensions per agent
  
  # Mask parameters
  mask_threshold: 0.5       # Threshold for converting continuous to binary mask
  temperature: 1.0          # Temperature for Gumbel-Softmax
  hard_gumbel: false        # Whether to use hard Gumbel-Softmax
  
  # Scheduler parameters
  scheduler_factor: 0.5     # Factor for learning rate reduction
  scheduler_patience: 10    # Patience for learning rate scheduler

gnn:
training:
  # Data source for training
  # IMPORTANT: Training should ALWAYS use reference trajectories
  data_source: "reference_trajectories"     # Use reference trajectories for training
  data_dir: "reference_trajectories_${N_agents}p"  # Directory containing reference trajectory data
  
  # Data split configuration
  train_samples: 384         # Number of samples to use for training (first N samples)
  test_samples: 128          # Number of samples to use for testing (later N samples)
  
  # General training settings
  seed: 42                  # Random seed for reproducibility
  use_gpu: true             # Whether to use GPU if available
  mixed_precision: false    # Whether to use mixed precision training
  
  # Data loading
  num_workers: 4            # Number of data loading workers
  prefetch_factor: 2        # Prefetch factor for data loading
  
  # Logging and monitoring
  log_interval: 10          # Interval for logging training metrics
  eval_interval: 50         # Interval for evaluation during training
  save_interval: 100        # Interval for saving checkpoints
  
  # TensorBoard logging
  tensorboard_log: true     # Whether to enable TensorBoard logging
  log_dir: "log"            # Base directory for logs
  
  # Model saving
  save_best_only: true      # Whether to save only the best model
  model_save_format: "pkl"  # Format for saving models ["pkl", "flax"]

testing:

# ============================================================================
# PATHS AND DIRECTORIES
# ============================================================================
paths:
  # Data directories
  # TRAINING DATA: Always use reference trajectories for training all models
  reference_data_dir: "src/data/reference_trajectories_${N_agents}p"           # For reference trajectory generation
  training_data_dir: "src/data/reference_trajectories_${N_agents}p"            # For goal inference and PSN training
  # goal_inference_data_dir: "reference_trajectories_${N_agents}p"      # Goal inference model training data
  
  # TESTING DATA: Different for different models
  # goal_inference_testing_data_dir: "reference_trajectories_${N_agents}p"      # Goal inference testing (tests goal prediction)
  psn_testing_data_dir: "src/data/receding_horizon_trajectories_${N_agents}p"          # PSN testing (tests player selection)
  testing_data_dir: "src/data/receding_horizon_trajectories_${N_agents}p"              # Default/legacy testing directory
  
  # Data split configuration (shared across all data loading)
  train_samples: 384         # Number of samples to use for training (first N samples)
  test_samples: 128          # Number of samples to use for testing (later N samples)

  # Model directories
  models_dir: "src/models"
  checkpoints_dir: "checkpoints"
  
  # Output directories
  results_dir: "results"
  logs_dir: "log"
  plots_dir: "plots"

# ============================================================================
# DEBUGGING AND DEVELOPMENT
# ============================================================================
debug:
  # Debug mode settings
  gradient_clip_value: 1.0  # Value for gradient clipping

# ============================================================================
# DEVICE CONFIGURATION
# ============================================================================
device:
  # Device selection
  preferred_device: "gpu"   # Preferred device ["gpu", "cpu", "auto"]
  
  # JAX configuration
  jax_platform_name: null  # JAX platform name (null = auto-detect)

