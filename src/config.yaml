game:
  dt: 0.1
  T_receding_horizon_planning: 20            # Planning horizon for each individual game
  T_receding_horizon_iterations: 50               # Total number of receding horizon iterations
  T_total: 50               # Total number of time steps in trajectory (T_receding_horizon_planning * T_receding_horizon_iterations)
  T_observation: 10         # Number of steps to observe before solving the game

  # Agent configuration
  N_agents: 10             # Total number of agents in the game
  ego_agent_id: 0           # ID of the ego agent (usually 0)
  
  # State and control dimensions
  state_dim: 4              # [x, y, vx, vy] for point mass agents
  control_dim: 2            # [ax, ay] for point mass agents
  
  # Environment parameters
  boundary_size_4p: 2.5
  boundary_size_10p: 3.5

  # initiation type (random positions or collisions at origin)
  initiation_type: "random" # ["random", "origin"]

optimization:
  # iLQGames solver parameters
  num_iters: 50            # Number of iterations for iLQGames solver  
  step_size: 0.005          # Step size for gradient descent
  
  # Cost function weights
  navigation_weight: 1.0    # Weight for navigation cost
  collision_weight: 1.0    # Weight for collision avoidance
  collision_scale: 1.0      # Scale factor in exponential collision cost
  control_weight: 0.1       # Weight for control cost

  # Q, R matrices
  Q: [0.1, 0.1, 0.001, 0.001]
  R: [0.01, 0.01]

reference_generation:
  num_samples: 1000
  reference_gen_type: "variable" # ["fixed", "variable"] where variable is number of agents is random up to n_agents
  save_dir_fixed: "reference_trajectories_${N_agents}p"
  save_dir_variable: "reference_trajectories_upto_${N_agents}p"

  train_samples: 0.9 # decimals encoding train/test split
  test_samples: 0.1

  # Visualization
  save_plots: true          # Whether to save trajectory plots
  plot_format: "png"        # Plot format ["png", "pdf", "svg"]
  plot_dpi: 300             # Plot resolution

psn:
  # Network architecture
  # Hidden layer dimensions: [64, 32] for 4 players, [128, 64] for 10 players (reduced for GRU version)
  hidden_dims_4p: [64, 32]          # Hidden dimensions for MLP head (reduced for GRU version)
  hidden_dims_10p: [128, 64]        # Hidden dimensions for MLP head (reduced for GRU version)
  gru_hidden_size: 64               # GRU hidden size for temporal processing
  output_dim_per_agent: 8   # Output dimension per other agent
  activation: "relu"        # Activation function
  use_batch_norm: false     # Disabled for GRU version (use dropout instead)
  dropout_rate: 0.3         # Dropout rate (increased for GRU version)
  
  # Training parameters
  learning_rate: 0.002      # Learning rate for Adam optimizer
  batch_size: 64            # Batch size for training
  num_epochs: 50            # Number of training epochs
  
  # Loss function weights
  sigma1: 0.075               # Weight for mask sparsity loss
  sigma2: 0.075               # Weight for mask binary loss
  
  # Goal source configuration
  # IMPORTANT: This setting affects testing behavior:
  # - If true: PSN training uses true goals, testing should also use true goals
  # - If false: PSN training uses goal inference model, testing should also use goal inference model
  # The testing script will automatically detect this setting to ensure consistency
  use_true_goals: true     # Whether to use true goals instead of predicted goals from goal inference model
  
  # Observation input options
  obs_input_type: "full"    # Observation input type ["full", "partial"]
  # - "full": Use full state (x, y, vx, vy) - 4 dimensions per agent
  # - "partial": Use position only (x, y) - 2 dimensions per agent
  
  # Mask parameters
  mask_threshold: 0.5       # Threshold for converting continuous to binary mask
  temperature: 1.0          # Temperature for Gumbel-Softmax
  hard_gumbel: false        # Whether to use hard Gumbel-Softmax
  
  # Scheduler parameters
  scheduler_factor: 0.5     # Factor for learning rate reduction
  scheduler_patience: 10    # Patience for learning rate scheduler

  # observation length
  observation_length: 10    # Length of observation history (K_observation)

gnn:
  # Training parameters
  learning_rate: 0.002      # Learning rate for Adam optimizer
  batch_size: 64            # Batch size for training
  num_epochs: 100            # Number of training epochs
  dropout_rate: 0.3

  # Loss function weights
  sigma1: 0.075               # Weight for mask sparsity loss
  sigma2: 0.075               # Weight for mask binary loss

  # graph edge creation parameters
  edge_metric: "fully_connected"

  # message passing rounds
  num_message_passing_rounds: 5

  # dimensions
  gru_hidden_size: 64
  message_mlp_dims: [128, 64]
  influence_head_dims: [128, 64]

training:
  # Data source for training
  # IMPORTANT: Training should ALWAYS use reference trajectories
  data_source: "reference_trajectories"     # Use reference trajectories for training
  data_dir: "reference_trajectories_${N_agents}p"  # Directory containing reference trajectory data
  
  # Data split configuration
  train_samples: 0.75         # Number of samples to use for training (first N samples)
  test_samples: 0.25          # Number of samples to use for testing (later N samples)
  
  # General training settings
  seed: 42                  # Random seed for reproducibility
  use_gpu: true             # Whether to use GPU if available
  mixed_precision: false    # Whether to use mixed precision training
  
  # Data loading
  num_workers: 4            # Number of data loading workers
  prefetch_factor: 2        # Prefetch factor for data loading
  
  # Logging and monitoring
  log_interval: 10          # Interval for logging training metrics
  eval_interval: 50         # Interval for evaluation during training
  save_interval: 100        # Interval for saving checkpoints
  
  # TensorBoard logging
  tensorboard_log: true     # Whether to enable TensorBoard logging
  log_dir: "log"            # Base directory for logs
  
  # Model saving
  save_best_only: true      # Whether to save only the best model
  model_save_format: "pkl"  # Format for saving models ["pkl", "flax"]

testing:
  # PSN testing data  
  psn_data_source: "receding_horizon_trajectories"
  psn_data_dir: "receding_horizon_trajectories_${N_agents}p"  # PSN tests on receding horizon trajectories
  
  # Data split configuration (shared across all testing)
  train_samples: 384         # Number of samples to use for training (first N samples)
  test_samples: 128          # Number of samples to use for testing (later N samples)
  
  # Legacy/general testing parameters (for backward compatibility)
  data_source: "receding_horizon_trajectories"  # Default for PSN testing
  data_dir: "receding_horizon_trajectories_${N_agents}p"  # Default directory
  
  # Test data parameters
  num_test_samples: 10      # Number of test samples to evaluate
  test_data_dir: "receding_horizon_trajectories_${N_agents}p"  # Directory with test data (DEPRECATED: use specific data_dir instead)

  # Default model paths (fallback for when templates can't be resolved)
  psn_model: null  # Will be set programmatically based on actual training parameters
  goal_inference_model: null  # Will be set programmatically based on actual training parameters
  
  # Receding horizon testing parameters
  receding_horizon:
    # Two-phase approach configuration
    initial_stabilization_iterations: 0  # No stabilization period - use PSN and goal inference from step 10
    
    # Player selection configuration for PSN testing (NOT used for baseline methods)
    # Selection method: "threshold" or "rank" - only applies when use_baseline=False
    # - "threshold": Select agents with mask values > mask_threshold (default behavior)
    # - "rank": Select top N agents based on mask values (rank = N means select N-1 other agents)
    # Note: Baseline methods use their own internal selection logic and ignore these parameters
    selection_method: "threshold"        # ["threshold", "rank"] - only for PSN methods
    mask_threshold: 0.5                  # Threshold for agent selection when using "threshold" method
    rank: 3                              # Number of total agents when using "rank" method (rank=3 means 3-player game, i.e., select 2 other agents)
    
    # Testing configuration
    num_samples: 10                     # Number of samples to test (use later 128 samples)
    use_later_samples: true             # Whether to use later samples (samples 384-511) instead of first samples
    output_dir: "receding_horizon_test_results"  # Test results will be placed under the PSN model directory
    
    # Baseline testing configuration
    use_baseline: false                  # Whether to use baseline methods instead of PSN model
    baseline_mode: "all"    # Baseline method ["All", "Distance Threshold", "Nearest Neighbor", "Jacobian", "Hessian", "Cost Evolution", "Barrier Function", "Control Barrier Function"]
    baseline_parameter: 3                # Parameter for baseline method (e.g., number of players (including ego), distance threshold)
    
    # Evaluation metrics configuration - automatically set based on test_type
    # prediction_test: compute_prediction_metrics=true, compute_planning_metrics=true
    # planning_test: compute_prediction_metrics=false, compute_planning_metrics=true
    compute_prediction_metrics: true     # Whether to compute prediction metrics (ADE, FDE) - auto-set based on test_type
    compute_planning_metrics: true       # Whether to compute planning metrics (nav_cost, safety_cost, control_cost) - always true
    
    # Visualization settings
    create_gif: false                     # Whether to create trajectory GIFs
    gif_duration: 0.5                    # Duration per frame in seconds
    gif_loop: 0                          # Number of loops (0 = infinite)
    
    # Performance monitoring
    track_computation_time: true         # Whether to track game solving time
    save_detailed_results: true          # Whether to save detailed iteration results
  
  # Evaluation metrics
  compute_trajectory_error: true    # Whether to compute trajectory tracking error
  compute_collision_rate: true      # Whether to compute collision statistics
  compute_goal_accuracy: true       # Whether to compute goal prediction accuracy
  
  # Visualization
  create_animations: true           # Whether to create trajectory animations
  save_comprehensive_plots: true   # Whether to save detailed analysis plots
  animation_fps: 10                 # FPS for animations
  
  # Output directories
  results_dir: "test_results"       # Directory for test results
  plots_dir: "test_plots"           # Directory for test plots 

# ============================================================================
# PATHS AND DIRECTORIES
# ============================================================================
paths:
  # Data directories
  # TRAINING DATA: Always use reference trajectories for training all models
  reference_data_dir: "src/data/reference_trajectories_${N_agents}p"           # For reference trajectory generation
  training_data_dir: "src/data/reference_trajectories_${N_agents}p"            # For goal inference and PSN training
  # goal_inference_data_dir: "reference_trajectories_${N_agents}p"      # Goal inference model training data
  
  # TESTING DATA: Different for different models
  # goal_inference_testing_data_dir: "reference_trajectories_${N_agents}p"      # Goal inference testing (tests goal prediction)
  psn_testing_data_dir: "src/data/receding_horizon_trajectories_${N_agents}p"          # PSN testing (tests player selection)
  testing_data_dir: "src/data/receding_horizon_trajectories_${N_agents}p"              # Default/legacy testing directory
  
  # Data split configuration (shared across all data loading)
  train_samples: 0.9         # Number of samples to use for training (first N samples)
  test_samples: 0.1          # Number of samples to use for testing (later N samples)

  # Model directories
  models_dir: "src/models"
  checkpoints_dir: "checkpoints"
  
  # Output directories
  results_dir: "results"
  logs_dir: "log"
  plots_dir: "plots"

# ============================================================================
# DEBUGGING AND DEVELOPMENT
# ============================================================================
debug:
  # Debug mode settings
  gradient_clip_value: 1.0  # Value for gradient clipping

# ============================================================================
# DEVICE CONFIGURATION
# ============================================================================
device:
  # Device selection
  preferred_device: "gpu"   # Preferred device ["gpu", "cpu", "auto"]
  
  # JAX configuration
  jax_platform_name: null  # JAX platform name (null = auto-detect)

